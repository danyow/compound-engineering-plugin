# DSPy.rb æµ‹è¯•ã€ä¼˜åŒ–ä¸å¯è§‚æµ‹æ€§

## æµ‹è¯•

DSPy.rb ä¸º LLM é€»è¾‘å¯ç”¨æ ‡å‡†çš„ RSpec æµ‹è¯•æ¨¡å¼ï¼Œä½¿æ‚¨çš„ AI åº”ç”¨å¯æµ‹è¯•å’Œå¯ç»´æŠ¤ã€‚

### åŸºæœ¬æµ‹è¯•è®¾ç½®

```ruby
require 'rspec'
require 'dspy'

RSpec.describe EmailClassifier do
  before do
    DSPy.configure do |c|
      c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
    end
  end

  describe '#classify' do
    it 'classifies technical support emails correctly' do
      classifier = EmailClassifier.new
      result = classifier.forward(
        email_subject: "Can't log in",
        email_body: "I'm unable to access my account"
      )

      expect(result[:category]).to eq('Technical')
      expect(result[:priority]).to be_in(['High', 'Medium', 'Low'])
    end
  end
end
```

### æ¨¡æ‹Ÿ LLM å“åº”

æ— éœ€è¿›è¡Œå®é™… API è°ƒç”¨å³å¯æµ‹è¯•æ‚¨çš„æ¨¡å—:

```ruby
RSpec.describe MyModule do
  it 'handles mock responses correctly' do
    # åˆ›å»ºä¸€ä¸ªè¿”å›é¢„å®šç»“æœçš„æ¨¡æ‹Ÿé¢„æµ‹å™¨
    mock_predictor = instance_double(DSPy::Predict)
    allow(mock_predictor).to receive(:forward).and_return({
      category: 'Technical',
      priority: 'High',
      confidence: 0.95
    })

    # å°†æ¨¡æ‹Ÿæ³¨å…¥åˆ°æ‚¨çš„æ¨¡å—ä¸­
    module_instance = MyModule.new
    module_instance.instance_variable_set(:@predictor, mock_predictor)

    result = module_instance.forward(input: 'test data')
    expect(result[:category]).to eq('Technical')
  end
end
```

### æµ‹è¯•ç±»å‹å®‰å…¨

éªŒè¯ç­¾åå¼ºåˆ¶æ‰§è¡Œç±»å‹çº¦æŸ:

```ruby
RSpec.describe EmailClassificationSignature do
  it 'validates output types' do
    predictor = DSPy::Predict.new(EmailClassificationSignature)

    # è¿™åº”è¯¥å¯ä»¥å·¥ä½œ
    result = predictor.forward(
      email_subject: 'Test',
      email_body: 'Test body'
    )
    expect(result[:category]).to be_a(String)

    # æµ‹è¯•æ— æ•ˆç±»å‹è¢«æ•è·
    expect {
      # æ¨¡æ‹Ÿ LLM è¿”å›æ— æ•ˆç±»å‹
      predictor.send(:validate_output, { category: 123 })
    }.to raise_error(DSPy::ValidationError)
  end
end
```

### æµ‹è¯•è¾¹ç¼˜æƒ…å†µ

å§‹ç»ˆæµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œé”™è¯¯åœºæ™¯:

```ruby
RSpec.describe EmailClassifier do
  it 'handles empty emails' do
    classifier = EmailClassifier.new
    result = classifier.forward(
      email_subject: '',
      email_body: ''
    )
    # å®šä¹‰è¾¹ç¼˜æƒ…å†µçš„é¢„æœŸè¡Œä¸º
    expect(result[:category]).to eq('General')
  end

  it 'handles very long emails' do
    long_body = 'word ' * 10000
    classifier = EmailClassifier.new

    expect {
      classifier.forward(
        email_subject: 'Test',
        email_body: long_body
      )
    }.not_to raise_error
  end

  it 'handles special characters' do
    classifier = EmailClassifier.new
    result = classifier.forward(
      email_subject: 'Test <script>alert("xss")</script>',
      email_body: 'Body with Ã©mojis ğŸ‰ and spÃ«cial Ã§haracters'
    )

    expect(result[:category]).to be_in(['Technical', 'Billing', 'General'])
  end
end
```

### é›†æˆæµ‹è¯•

ç«¯åˆ°ç«¯æµ‹è¯•å®Œæ•´å·¥ä½œæµ:

```ruby
RSpec.describe EmailProcessingPipeline do
  it 'processes email through complete pipeline' do
    pipeline = EmailProcessingPipeline.new

    result = pipeline.forward(
      email_subject: 'Billing question',
      email_body: 'How do I update my payment method?'
    )

    # éªŒè¯å®Œæ•´çš„ç®¡é“è¾“å‡º
    expect(result[:classification]).to eq('Billing')
    expect(result[:priority]).to eq('Medium')
    expect(result[:suggested_response]).to include('payment')
    expect(result[:assigned_team]).to eq('billing_support')
  end
end
```

### ä½¿ç”¨ VCR è¿›è¡Œç¡®å®šæ€§æµ‹è¯•

ä½¿ç”¨ VCR è®°å½•å’Œé‡æ”¾ API å“åº”:

```ruby
require 'vcr'

VCR.configure do |config|
  config.cassette_library_dir = 'spec/vcr_cassettes'
  config.hook_into :webmock
  config.filter_sensitive_data('<OPENAI_API_KEY>') { ENV['OPENAI_API_KEY'] }
end

RSpec.describe EmailClassifier do
  it 'classifies emails consistently', :vcr do
    VCR.use_cassette('email_classification') do
      classifier = EmailClassifier.new
      result = classifier.forward(
        email_subject: 'Test subject',
        email_body: 'Test body'
      )

      expect(result[:category]).to eq('Technical')
    end
  end
end
```

## ä¼˜åŒ–

DSPy.rb æä¾›å¼ºå¤§çš„ä¼˜åŒ–åŠŸèƒ½æ¥è‡ªåŠ¨æ”¹è¿›æ‚¨çš„æç¤ºè¯å’Œæ¨¡å—ã€‚

### MIPROv2 ä¼˜åŒ–

MIPROv2 æ˜¯ä¸€ç§é«˜çº§å¤šæç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿ç”¨ bootstrap é‡‡æ ·ã€æŒ‡ä»¤ç”Ÿæˆå’Œè´å¶æ–¯ä¼˜åŒ–ã€‚

```ruby
require 'dspy/mipro'

# å®šä¹‰è¦ä¼˜åŒ–çš„æ¨¡å—
class EmailClassifier < DSPy::Module
  def initialize
    super
    @predictor = DSPy::ChainOfThought.new(EmailClassificationSignature)
  end

  def forward(input)
    @predictor.forward(input)
  end
end

# å‡†å¤‡è®­ç»ƒæ•°æ®
training_examples = [
  {
    input: { email_subject: "Can't log in", email_body: "Password reset not working" },
    expected_output: { category: 'Technical', priority: 'High' }
  },
  {
    input: { email_subject: "Billing question", email_body: "How much does premium cost?" },
    expected_output: { category: 'Billing', priority: 'Medium' }
  },
  # æ·»åŠ æ›´å¤šç¤ºä¾‹...
]

# å®šä¹‰è¯„ä¼°æŒ‡æ ‡
def accuracy_metric(example, prediction)
  (example[:expected_output][:category] == prediction[:category]) ? 1.0 : 0.0
end

# è¿è¡Œä¼˜åŒ–
optimizer = DSPy::MIPROv2.new(
  metric: method(:accuracy_metric),
  num_candidates: 10,
  num_threads: 4
)

optimized_module = optimizer.compile(
  EmailClassifier.new,
  trainset: training_examples
)

# ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å—
result = optimized_module.forward(
  email_subject: "New email",
  email_body: "New email content"
)
```

### Bootstrap Few-Shot å­¦ä¹ 

ä»æ‚¨çš„è®­ç»ƒæ•°æ®è‡ªåŠ¨ç”Ÿæˆ few-shot ç¤ºä¾‹:

```ruby
require 'dspy/teleprompt'

# åˆ›å»ºç”¨äº few-shot ä¼˜åŒ–çš„ teleprompter
teleprompter = DSPy::BootstrapFewShot.new(
  metric: method(:accuracy_metric),
  max_bootstrapped_demos: 5,
  max_labeled_demos: 3
)

# ç¼–è¯‘ä¼˜åŒ–åçš„æ¨¡å—
optimized = teleprompter.compile(
  MyModule.new,
  trainset: training_examples
)
```

### è‡ªå®šä¹‰ä¼˜åŒ–æŒ‡æ ‡

ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹å®šä¹‰è‡ªå®šä¹‰æŒ‡æ ‡:

```ruby
def custom_metric(example, prediction)
  score = 0.0

  # ç±»åˆ«å‡†ç¡®æ€§ (60% æƒé‡)
  score += 0.6 if example[:expected_output][:category] == prediction[:category]

  # ä¼˜å…ˆçº§å‡†ç¡®æ€§ (40% æƒé‡)
  score += 0.4 if example[:expected_output][:priority] == prediction[:priority]

  score
end

# åœ¨ä¼˜åŒ–ä¸­ä½¿ç”¨
optimizer = DSPy::MIPROv2.new(
  metric: method(:custom_metric),
  num_candidates: 10
)
```

### A/B æµ‹è¯•ä¸åŒæ–¹æ³•

æ¯”è¾ƒä¸åŒçš„æ¨¡å—å®ç°:

```ruby
# æ–¹æ³• A: ChainOfThought
class ApproachA < DSPy::Module
  def initialize
    super
    @predictor = DSPy::ChainOfThought.new(EmailClassificationSignature)
  end

  def forward(input)
    @predictor.forward(input)
  end
end

# æ–¹æ³• B: å¸¦å·¥å…·çš„ ReAct
class ApproachB < DSPy::Module
  def initialize
    super
    @predictor = DSPy::ReAct.new(
      EmailClassificationSignature,
      tools: [KnowledgeBaseTool.new]
    )
  end

  def forward(input)
    @predictor.forward(input)
  end
end

# è¯„ä¼°ä¸¤ç§æ–¹æ³•
def evaluate_approach(approach_class, test_set)
  approach = approach_class.new
  scores = test_set.map do |example|
    prediction = approach.forward(example[:input])
    accuracy_metric(example, prediction)
  end
  scores.sum / scores.size
end

approach_a_score = evaluate_approach(ApproachA, test_examples)
approach_b_score = evaluate_approach(ApproachB, test_examples)

puts "Approach A accuracy: #{approach_a_score}"
puts "Approach B accuracy: #{approach_b_score}"
```

## å¯è§‚æµ‹æ€§

è·Ÿè¸ª LLM åº”ç”¨åœ¨ç”Ÿäº§ä¸­çš„æ€§èƒ½ã€token ä½¿ç”¨å’Œè¡Œä¸ºã€‚

### OpenTelemetry é›†æˆ

é…ç½®åï¼ŒDSPy.rb ä¼šè‡ªåŠ¨ä¸ OpenTelemetry é›†æˆ:

```ruby
require 'opentelemetry/sdk'
require 'dspy'

# é…ç½® OpenTelemetry
OpenTelemetry::SDK.configure do |c|
  c.service_name = 'my-dspy-app'
  c.use_all # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ç›‘æµ‹å·¥å…·
end

# DSPy è‡ªåŠ¨ä¸ºé¢„æµ‹åˆ›å»ºè¿½è¸ª
predictor = DSPy::Predict.new(MySignature)
result = predictor.forward(input: 'data')
# è¿½è¸ªè‡ªåŠ¨å‘é€åˆ°æ‚¨çš„ OpenTelemetry æ”¶é›†å™¨
```

### Langfuse é›†æˆ

ä½¿ç”¨ Langfuse è·Ÿè¸ªè¯¦ç»†çš„ LLM æ‰§è¡Œè¿½è¸ª:

```ruby
require 'dspy/langfuse'

# é…ç½® Langfuse
DSPy.configure do |c|
  c.lm = DSPy::LM.new('openai/gpt-4o-mini', api_key: ENV['OPENAI_API_KEY'])
  c.langfuse = {
    public_key: ENV['LANGFUSE_PUBLIC_KEY'],
    secret_key: ENV['LANGFUSE_SECRET_KEY'],
    host: ENV['LANGFUSE_HOST'] || 'https://cloud.langfuse.com'
  }
end

# æ‰€æœ‰é¢„æµ‹éƒ½ä¼šè‡ªåŠ¨è¿½è¸ª
predictor = DSPy::Predict.new(MySignature)
result = predictor.forward(input: 'data')
# åœ¨ Langfuse ä»ªè¡¨æ¿ä¸­æŸ¥çœ‹è¯¦ç»†è¿½è¸ª
```

### æ‰‹åŠ¨ Token è·Ÿè¸ª

æ— éœ€å¤–éƒ¨æœåŠ¡å³å¯è·Ÿè¸ª token ä½¿ç”¨:

```ruby
class TokenTracker
  def initialize
    @total_tokens = 0
    @request_count = 0
  end

  def track_prediction(predictor, input)
    start_time = Time.now
    result = predictor.forward(input)
    duration = Time.now - start_time

    # ä»å“åº”å…ƒæ•°æ®è·å– token ä½¿ç”¨
    tokens = result.metadata[:usage][:total_tokens] rescue 0
    @total_tokens += tokens
    @request_count += 1

    puts "Request ##{@request_count}: #{tokens} tokens in #{duration}s"
    puts "Total tokens used: #{@total_tokens}"

    result
  end
end

# ä½¿ç”¨
tracker = TokenTracker.new
predictor = DSPy::Predict.new(MySignature)

result = tracker.track_prediction(predictor, { input: 'data' })
```

### è‡ªå®šä¹‰æ—¥å¿—

å‘æ‚¨çš„æ¨¡å—æ·»åŠ è¯¦ç»†æ—¥å¿—:

```ruby
class EmailClassifier < DSPy::Module
  def initialize
    super
    @predictor = DSPy::ChainOfThought.new(EmailClassificationSignature)
    @logger = Logger.new(STDOUT)
  end

  def forward(input)
    @logger.info "Classifying email: #{input[:email_subject]}"

    start_time = Time.now
    result = @predictor.forward(input)
    duration = Time.now - start_time

    @logger.info "Classification: #{result[:category]} (#{duration}s)"

    if result[:reasoning]
      @logger.debug "Reasoning: #{result[:reasoning]}"
    end

    result
  rescue => e
    @logger.error "Classification failed: #{e.message}"
    raise
  end
end
```

### æ€§èƒ½ç›‘æ§

ç›‘æ§å»¶è¿Ÿå’Œæ€§èƒ½æŒ‡æ ‡:

```ruby
class PerformanceMonitor
  def initialize
    @metrics = {
      total_requests: 0,
      total_duration: 0.0,
      errors: 0,
      success_count: 0
    }
  end

  def monitor_request
    start_time = Time.now
    @metrics[:total_requests] += 1

    begin
      result = yield
      @metrics[:success_count] += 1
      result
    rescue => e
      @metrics[:errors] += 1
      raise
    ensure
      duration = Time.now - start_time
      @metrics[:total_duration] += duration

      if @metrics[:total_requests] % 10 == 0
        print_stats
      end
    end
  end

  def print_stats
    avg_duration = @metrics[:total_duration] / @metrics[:total_requests]
    success_rate = @metrics[:success_count].to_f / @metrics[:total_requests]

    puts "\n=== Performance Stats ==="
    puts "Total requests: #{@metrics[:total_requests]}"
    puts "Average duration: #{avg_duration.round(3)}s"
    puts "Success rate: #{(success_rate * 100).round(2)}%"
    puts "Errors: #{@metrics[:errors]}"
    puts "========================\n"
  end
end

# ä½¿ç”¨
monitor = PerformanceMonitor.new
predictor = DSPy::Predict.new(MySignature)

result = monitor.monitor_request do
  predictor.forward(input: 'data')
end
```

### é”™è¯¯ç‡è·Ÿè¸ª

ç›‘æ§é”™è¯¯ç‡å¹¶å‘å‡ºè­¦æŠ¥:

```ruby
class ErrorRateMonitor
  def initialize(alert_threshold: 0.1)
    @alert_threshold = alert_threshold
    @recent_results = []
    @window_size = 100
  end

  def track_result(success:)
    @recent_results << success
    @recent_results.shift if @recent_results.size > @window_size

    error_rate = calculate_error_rate
    alert_if_needed(error_rate)

    error_rate
  end

  private

  def calculate_error_rate
    failures = @recent_results.count(false)
    failures.to_f / @recent_results.size
  end

  def alert_if_needed(error_rate)
    if error_rate > @alert_threshold
      puts "âš ï¸  ALERT: Error rate #{(error_rate * 100).round(2)}% exceeds threshold!"
      # å‘é€é€šçŸ¥ã€å‘¼å«å¾…å‘½äººå‘˜ç­‰
    end
  end
end
```

## æœ€ä½³å®è·µ

### 1. ä»æµ‹è¯•å¼€å§‹

åœ¨ä¼˜åŒ–ä¹‹å‰ç¼–å†™æµ‹è¯•:

```ruby
# é¦–å…ˆå®šä¹‰æµ‹è¯•ç”¨ä¾‹
test_cases = [
  { input: {...}, expected: {...} },
  # æ›´å¤šæµ‹è¯•ç”¨ä¾‹...
]

# ç¡®ä¿åŸºçº¿åŠŸèƒ½
test_cases.each do |tc|
  result = module.forward(tc[:input])
  assert result[:category] == tc[:expected][:category]
end

# ç„¶åä¼˜åŒ–
optimized = optimizer.compile(module, trainset: test_cases)
```

### 2. ä½¿ç”¨æœ‰æ„ä¹‰çš„æŒ‡æ ‡

å®šä¹‰ä¸ä¸šåŠ¡ç›®æ ‡ä¸€è‡´çš„æŒ‡æ ‡:

```ruby
def business_aligned_metric(example, prediction)
  # é«˜ä¼˜å…ˆçº§é”™è¯¯æˆæœ¬æ›´é«˜
  if example[:expected_output][:priority] == 'High'
    return prediction[:priority] == 'High' ? 1.0 : 0.0
  else
    return prediction[:category] == example[:expected_output][:category] ? 0.8 : 0.0
  end
end
```

### 3. åœ¨ç”Ÿäº§ä¸­ç›‘æ§

å§‹ç»ˆè·Ÿè¸ªç”Ÿäº§æ€§èƒ½:

```ruby
class ProductionModule < DSPy::Module
  def initialize
    super
    @predictor = DSPy::ChainOfThought.new(MySignature)
    @monitor = PerformanceMonitor.new
    @error_tracker = ErrorRateMonitor.new
  end

  def forward(input)
    @monitor.monitor_request do
      result = @predictor.forward(input)
      @error_tracker.track_result(success: true)
      result
    rescue => e
      @error_tracker.track_result(success: false)
      raise
    end
  end
end
```

### 4. ç‰ˆæœ¬åŒ–æ‚¨çš„æ¨¡å—

è·Ÿè¸ªéƒ¨ç½²çš„æ¨¡å—ç‰ˆæœ¬:

```ruby
class EmailClassifierV2 < DSPy::Module
  VERSION = '2.1.0'

  def initialize
    super
    @predictor = DSPy::ChainOfThought.new(EmailClassificationSignature)
  end

  def forward(input)
    result = @predictor.forward(input)
    result.merge(model_version: VERSION)
  end
end
```
